{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dict={'14783134 15697333 14854817 14925479':0,'14847385 14844587 14848641 14847398':1, \\\n",
    "#      '14786237 15697082 14722731 14924977':2, '14844093 15705739 14854331 15699885':3, \\\n",
    "#      '15632285 15706536 14721977 14925219':4, '15630486 15702410 14718849 15709093':5, \\\n",
    "#      '14924216 14781104 14717848 14791612':6, '14782903 15634620 15638402 15706300':7, \\\n",
    "#     '14858934 15636660 15704193 14849963':8, '15709098 14716590 14924703 14779559':9, \\\n",
    "#      '14726332 14728344 14854542 14844591':10,'14856354 14844592':11, \\\n",
    "#      '15710359 14847407 14845602 14859696':12, '14794687 14782344':13, \\\n",
    "#      '14925756 15639967 14853254 14728639':14, '14844593 14924945':15, \\\n",
    "#     '14844856 14724258 14925237 14854807':16, '14852788 14717848 15639958 15632020':17, \\\n",
    "#      '14784131 14858934 14784131 14845064':18}\n",
    "\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import gc\n",
    "\n",
    "# print('loading datasets......')\n",
    "# train_data = pd.read_csv('./data/train.csv')\n",
    "# new_label = []\n",
    "# for i in range(len(train_data)):\n",
    "#      new_label.append(label_dict[train_data.loc[i][\"label\"]])\n",
    "\n",
    "# train_data[\"new_label\"] = new_label\n",
    "# train_data.to_csv(\"./data/new_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading datasets......\n",
      "9819 lines in train datasets\n",
      "1800 lines in test datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "# handle raw data\n",
    "\n",
    "print('loading datasets......')\n",
    "train_data = pd.read_csv('../data/more_data.csv')\n",
    "test_data = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print('{} lines in train datasets'.format(len(train_data)))\n",
    "print('{} lines in test datasets'.format(len(test_data)))\n",
    "\n",
    "all_text = []\n",
    "\n",
    "for row in range(len(train_data)):\n",
    "    all_text.append(train_data.loc[row][\"name\"].split())\n",
    "    all_text.append(train_data.loc[row][\"description\"].split())\n",
    "\n",
    "for row in range(len(test_data)):\n",
    "    all_text.append(test_data.loc[row][\"name\"].split())\n",
    "    all_text.append(test_data.loc[row][\"description\"].split())\n",
    "\n",
    "with open(\"../data/raw_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "embed_size = 100\n",
    "window = 4\n",
    "min_count = 1\n",
    "\n",
    "model = Word2Vec(all_text, vector_size=embed_size, window=window, min_count=min_count, workers=4)\n",
    "model.wv.save_word2vec_format(\"../data/more_data_wordvec_embsize_{}_window_{}_mincount_{}.bin\".format(embed_size, window, min_count), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.word2vec import Word2Vec, KeyedVectors\n",
    "# embedding_model = KeyedVectors.load_word2vec_format(\"../data/glove_word2vec_100_4.bin\", binary=False).key_to_index\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# train_data = pd.read_csv(\"../data/new_train_data.csv\")\n",
    "# new_train = pd.DataFrame(columns=[\"name\", \"description\", \"new_label\"])\n",
    "# count = 0\n",
    "# for i in range(len(train_data)):\n",
    "#     flag = 0\n",
    "#     for name in train_data.iloc[i][\"name\"].split():\n",
    "#         if name in embedding_model:\n",
    "#             flag += 1\n",
    "#             break\n",
    "    \n",
    "#     for desc in train_data.iloc[i][\"description\"].split():\n",
    "#         if desc in embedding_model:\n",
    "#             flag += 1\n",
    "#             break\n",
    "#     if flag == 2:\n",
    "#         # print(\"sadas\")\n",
    "#         new_train.loc[len(new_train.index)] = [train_data.iloc[i][\"name\"], train_data.iloc[i][\"description\"], train_data.iloc[i][\"new_label\"]]\n",
    "#         # new_train.append(train_data.iloc[i][[\"name\", \"description\", \"new_label\"]], ignore_index = True)\n",
    "#         # print(i)\n",
    "#         # print(train_data.iloc[i])\n",
    "#         count += 1\n",
    "# print(count)\n",
    "# new_train.to_csv(\"./glove_word2vec_100_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2564e1fcdf75c3b5f6e3f6bbb639251a5c9cf0f98c916a2d724c13558ff7673d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
