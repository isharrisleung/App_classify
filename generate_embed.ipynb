{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label_dict={'14783134 15697333 14854817 14925479':0,'14847385 14844587 14848641 14847398':1, \\\n",
    "#      '14786237 15697082 14722731 14924977':2, '14844093 15705739 14854331 15699885':3, \\\n",
    "#      '15632285 15706536 14721977 14925219':4, '15630486 15702410 14718849 15709093':5, \\\n",
    "#      '14924216 14781104 14717848 14791612':6, '14782903 15634620 15638402 15706300':7, \\\n",
    "#     '14858934 15636660 15704193 14849963':8, '15709098 14716590 14924703 14779559':9, \\\n",
    "#      '14726332 14728344 14854542 14844591':10,'14856354 14844592':11, \\\n",
    "#      '15710359 14847407 14845602 14859696':12, '14794687 14782344':13, \\\n",
    "#      '14925756 15639967 14853254 14728639':14, '14844593 14924945':15, \\\n",
    "#     '14844856 14724258 14925237 14854807':16, '14852788 14717848 15639958 15632020':17, \\\n",
    "#      '14784131 14858934 14784131 14845064':18}\n",
    "\n",
    "# import pandas as pd\n",
    "# import pickle\n",
    "# import gc\n",
    "\n",
    "# print('loading datasets......')\n",
    "# train_data = pd.read_csv('./data/train.csv')\n",
    "# new_label = []\n",
    "# for i in range(len(train_data)):\n",
    "#      new_label.append(label_dict[train_data.loc[i][\"label\"]])\n",
    "\n",
    "# train_data[\"new_label\"] = new_label\n",
    "# train_data.to_csv(\"./data/new_train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading datasets......\n",
      "4199 lines in train datasets\n",
      "1800 lines in test datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import gc\n",
    "# handle raw data\n",
    "\n",
    "print('loading datasets......')\n",
    "train_data = pd.read_csv('./data/train.csv')\n",
    "test_data = pd.read_csv('./data/test.csv')\n",
    "\n",
    "print('{} lines in train datasets'.format(len(train_data)))\n",
    "print('{} lines in test datasets'.format(len(test_data)))\n",
    "\n",
    "all_text = []\n",
    "\n",
    "for row in range(len(train_data)):\n",
    "    all_text.append(train_data.loc[row][\"name\"].split())\n",
    "    all_text.append(train_data.loc[row][\"description\"].split())\n",
    "\n",
    "for row in range(len(test_data)):\n",
    "    all_text.append(test_data.loc[row][\"name\"].split())\n",
    "    all_text.append(test_data.loc[row][\"description\"].split())\n",
    "\n",
    "with open(\"./data/raw_text.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "embed_size = 100\n",
    "window = 4\n",
    "min_count = 2\n",
    "\n",
    "model = Word2Vec(all_text, vector_size=embed_size, window=window, min_count=min_count, workers=4)\n",
    "model.wv.save_word2vec_format(\"./data/wordvec_embsize_{}_window_{}_mincount_{}.bin\".format(embed_size, window, min_count), binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2635868 ,  0.7220004 ,  0.75459373, ..., -1.0276067 ,\n",
       "        -0.46086165, -0.55136937],\n",
       "       [ 0.11223569,  0.8719089 ,  0.79327935, ..., -0.99944395,\n",
       "         0.01496161,  0.03844398],\n",
       "       [-1.3938415 ,  0.44535938,  0.35238782, ..., -1.6259888 ,\n",
       "         1.0312802 , -0.18296252],\n",
       "       ...,\n",
       "       [-0.03802943,  0.04276467,  0.00815903, ..., -0.05578184,\n",
       "         0.03906186,  0.00302787],\n",
       "       [-0.0226745 ,  0.04254106,  0.01782553, ..., -0.01826478,\n",
       "         0.01883474, -0.02656068],\n",
       "       [-0.02182246,  0.01409838,  0.01010036, ...,  0.00467927,\n",
       "         0.00857336,  0.00279228]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim.models.word2vec import Word2Vec, KeyedVectors\n",
    "# embedding_model = KeyedVectors.load_word2vec_format(\"./data/wordvec_embsize_100_window_4_mincount_3.bin\", binary=True).key_to_index\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# train_data = pd.read_csv(\"./data/new_train_data.csv\")\n",
    "# new_train = pd.DataFrame(columns=[\"name\", \"description\", \"new_label\"])\n",
    "# for i in range(len(train_data)):\n",
    "#     flag = 0\n",
    "#     for name in train_data.iloc[i][\"name\"].split():\n",
    "#         if name in embedding_model:\n",
    "#             flag += 1\n",
    "#             break\n",
    "    \n",
    "#     for desc in train_data.iloc[i][\"description\"].split():\n",
    "#         if desc in embedding_model:\n",
    "#             flag += 1\n",
    "#             break\n",
    "#     if flag == 2:\n",
    "#         print(\"sadas\")\n",
    "#         new_train.append(train_data.iloc[i][[\"name\", \"description\", \"new_label\"]], ignore_index = True)\n",
    "# new_train.to_csv(\"./data/new_new_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2564e1fcdf75c3b5f6e3f6bbb639251a5c9cf0f98c916a2d724c13558ff7673d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
